{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, Normalizer, binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in both `/data/patients.tsv` and `/data/gene_expression.tsv`. We assume that `patients.tsv` has two columns for `sample` and `gleason_score` and that `gene_expression.tsv` is row-ordered according to genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in patient data.\n",
    "patients = pd.read_csv('/data/patients.tsv',\n",
    "    sep=\"\\t\",\n",
    "    usecols=['sample', 'gleason_score'],\n",
    "    index_col=0)\n",
    "\n",
    "# Read in gene expression data.\n",
    "gene_expression = pd.read_csv('/data/gene_expression.tsv',\n",
    "    sep=\"\\t\",\n",
    "    index_col=\"gene_id\")\n",
    "\n",
    "# Transpose gene expression data it so we can join with patients. We also\n",
    "# group by \"index\", take the first record so that we remove any duplicate\n",
    "# patients.\n",
    "gene_expression = gene_expression.\\\n",
    "    T.\\\n",
    "    reset_index().\\\n",
    "    groupby(\"index\").\\\n",
    "    first()\n",
    "\n",
    "# Rename our index to sample. Now our patients and gene expression data frames\n",
    "# have the same index names.\n",
    "gene_expression.index.rename(\"sample\", inplace=True)\n",
    "\n",
    "# Use only the first 12 characters in the sample id -- the rest is unknown.\n",
    "gene_expression.index = gene_expression.index.str.slice(0,12)\n",
    "\n",
    "# Join our tables.\n",
    "patient_gene_expression = patients.join(gene_expression)\n",
    "\n",
    "# Split our Dataframe into data we're going to train on.\n",
    "X = patient_gene_expression.drop('gleason_score').as_matrix()\n",
    "Y = patient_gene_expression.gleason_score >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our pipeline that we're going to use for hyperparameter selection,\n",
    "# cross-validation, and model building.\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "    ('normalizer', Normalizer()),\n",
    "    ('classifier', SGDClassifier(loss='log', penalty='elasticnet'))\n",
    "])\n",
    "\n",
    "# Specify parameter distributions that we're going to search across.\n",
    "parameter_grid = {\n",
    "    \"classifier__alpha\": np.logspace(-6, 1, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a grid search across the parameter distribution for our pipeline.\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "    param_grid=parameter_grid,\n",
    "    n_jobs=8,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation_results_list = []\n",
    "grid_search_results_list = []\n",
    "support_list = []\n",
    "\n",
    "# Iterate through 6 stratified k-folds\n",
    "for fold, (train, test) in enumerate(StratifiedKFold(Y, n_folds=6)):\n",
    "\n",
    "    print(\"Iterating through fold #{} of 6.\".format(fold+1))\n",
    "\n",
    "    # Search for best parameters using training data. \n",
    "    grid_search.fit(X[train], Y[train])\n",
    "\n",
    "    # Save grid search parameters\n",
    "    for grid_score in grid_search.grid_scores_:\n",
    "        grid_search_result = pd.Series(grid_score.parameters)\n",
    "        grid_search_result['score'] = grid_score.mean_validation_score\n",
    "        grid_search_result['fold'] = fold\n",
    "        grid_search_results_list.append(grid_search_result)\n",
    "\n",
    "    # Select the best estimator.\n",
    "    model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the list of supports selected from the feature_selection step\n",
    "    support = {\n",
    "        'fold': fold,\n",
    "        'support': model.named_steps['feature_selection'].get_support()\n",
    "    }\n",
    "\n",
    "    # Add grid search params to our support.\n",
    "    support.update(grid_search.best_params_)\n",
    "\n",
    "    # Append this to our list of supports.\n",
    "    support_list.append(support)\n",
    "\n",
    "    # Make predictions for the output.\n",
    "    probabilities = model.predict_proba(X[test])\n",
    "\n",
    "    # Calculate false/true positive rates\n",
    "    false_positive_rate, true_positive_rate, roc_thresholds = roc_curve(Y[test], probabilities[:, 1])\n",
    "\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(Y[test], probabilities[:, 1])\n",
    "\n",
    "    metrics = {\n",
    "        'fold': fold+1,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'area_under_curve': auc(false_positive_rate, true_positive_rate),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_thresholds': roc_thresholds,\n",
    "        'precision_recall_thresholds': pr_thresholds\n",
    "    }\n",
    "\n",
    "    # Add our hyperparameters to our results.\n",
    "    metrics.update(grid_search.best_params_)\n",
    "\n",
    "    # Add our results to the data frame so that we can track parameters and \n",
    "    cross_validation_results_list.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert our results to data frames for easy processing.\n",
    "support_results = pd.DataFrame(support_list)\n",
    "cross_validation_results = pd.DataFrame(cross_validation_results_list)\n",
    "grid_search_results = pd.DataFrame(grid_search_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a figure containing a subplot for each fold where we will visualize\n",
    "# hyperparameters selection.\n",
    "fig, axes = plt.subplots(2, 3, sharex='col', sharey='row')\n",
    "\n",
    "for fold, ax in enumerate(axes.flatten()):\n",
    "\n",
    "    # Look at the search results for this fold.\n",
    "    fold_grid_search_results = grid_search_results[grid_search_results.fold == fold].\\\n",
    "        drop('fold', 1).\\\n",
    "        pivot('random_forest__max_depth', 'random_forest__max_features')\n",
    "    \n",
    "    x, y = meshgrid(fold_grid_search_results.columns.levels[1].values,\n",
    "            fold_grid_search_results.index.values)\n",
    "\n",
    "    z = fold_grid_search_results.values\n",
    "\n",
    "    ax.contourf(x, y, z)\n",
    "\n",
    "    ax.set_xscale('log', basex=2)\n",
    "\n",
    "fig.suptitle(\"Random Forest Grid Search Results Per Fold\")\n",
    "fig.text(0.5, 0.02, 'Feature Count', ha='center')\n",
    "fig.text(0.04, 0.5, 'Depth', va='center', rotation='vertical')\n",
    "fig.savefig(\"random-forest-parameters-per-fold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, plot our ROC curves for each fold.\n",
    "fig = figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "for _, row in cross_validation_results.iterrows():\n",
    "    ax.plot(row.false_positive_rate, row.true_positive_rate)\n",
    "\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "fig.suptitle(\"Receiver operating characteristic curve per fold\")\n",
    "\n",
    "fig.savefig(\"roc-curve-per-fold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "for _, row in cross_validation_results.iterrows():\n",
    "    ax.plot(row.precision_recall_thresholds, row.precision[:-1], 'r')\n",
    "    ax.plot(row.precision_recall_thresholds, row.recall[:-1], 'b')\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Precision / Recall\")\n",
    "fig.suptitle(\"Precision and Recall vs Threshold per Fold\")\n",
    "\n",
    "fig.savefig(\"precision-recall-vs-threshold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, perform a grid search using all available data.\n",
    "models = grid_search.fit(X, Y)\n",
    "\n",
    "with open('pipeline.pickle', 'wb') as f:\n",
    "    pickle.dump(models.best_estimator_, 'model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
